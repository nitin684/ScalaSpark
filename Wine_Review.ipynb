{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://FireBall:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1548607726347)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\r\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}\r\n",
       "import org.apache.spark.ml.regression.GBTRegressor\r\n",
       "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\r\n",
       "import org.apache.spark.sql.{Encoders, SparkSession}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.{Encoders, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaStruct: org.apache.spark.sql.types.StructType = StructType(StructField(country,StringType,true), StructField(points,DoubleType,true), StructField(price,DoubleType,true))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schemaStruct = StructType(\n",
    "            StructField(\"country\", StringType) ::\n",
    "            StructField(\"points\", DoubleType) ::\n",
    "            StructField(\"price\", DoubleType) :: Nil\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, country: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "      .option(\"header\", true)\n",
    "      //.schema(schemaStruct)\n",
    "      .option(\"inferSchema\", true)\n",
    "      .csv(\"file:///C:/Users/FireBall/IdeaProjects/SparkDemo/src/main/data/winereviews/wine-data.csv\")\n",
    "      .na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_df: org.apache.spark.sql.DataFrame = [country: string, points: string ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val new_df = df.drop(\"id\", \"description\", \"designation\", \"province\", \"region_1\", \"region_2\", \"variety\", \"winery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- points: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: new_df.type = [country: string, points: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.printSchema()\n",
    "new_df.cache()\n",
    "//df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.sql.Row] = Array([US,96,235], [US,96,90])\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)\n",
    "//df.cache()\n",
    "//df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "import spark.implicits._\n",
    "case class Initial(country: String , points: String , price: String )\n",
    "case class Final(country: String, points: Double, price: Double)\n",
    "\n",
    "def swapType(in: Initial) = Final(\n",
    "    in.country,\n",
    "    in.points.toDouble,\n",
    "    in.price.toDouble\n",
    ")\n",
    "\n",
    "val df_2 = new_df.as[Initial].map(swapType(_))\n",
    "df_2.printSchema()\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "//new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df: org.apache.spark.sql.DataFrame = [country: string, points: double ... 1 more field]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_df = new_df.select( new_df(\"country\"), \n",
    "                              new_df(\"points\").cast(DoubleType).as(\"points\"), \n",
    "                              new_df(\"price\").cast(DoubleType).as(\"price\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.DataFrame = [country: string, points: double ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|country|points|price|\n",
      "+-------+------+-----+\n",
      "|     US|  96.0|235.0|\n",
      "|     US|  96.0| 90.0|\n",
      "|     US|  96.0| 65.0|\n",
      "|     US|  95.0| 65.0|\n",
      "|     US|  95.0| 60.0|\n",
      "|     US|  95.0| 48.0|\n",
      "|     US|  95.0| 48.0|\n",
      "|     US|  95.0|185.0|\n",
      "|     US|  95.0| 90.0|\n",
      "|     US|  95.0|325.0|\n",
      "|     US|  95.0| 75.0|\n",
      "|     US|  95.0| 24.0|\n",
      "|     US|  95.0| 60.0|\n",
      "|     US|  95.0| 45.0|\n",
      "|     US|  94.0|105.0|\n",
      "|     US|  94.0| 60.0|\n",
      "|     US|  94.0| 60.0|\n",
      "|     US|  90.0| 37.0|\n",
      "|     US|  90.0| 42.0|\n",
      "|     US|  90.0| 60.0|\n",
      "+-------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country: string, points: double ... 1 more field]\r\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [country: string, points: double ... 1 more field]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainingData, testData) = train_df.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelColumn: String = price\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelColumn = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countryIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_4d94574d0867\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countryIndexer = new StringIndexer()\n",
    "            .setInputCol(\"country\")\n",
    "            .setOutputCol(\"countryIndex\")\n",
    "            .setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_baa0c40228de\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(Array(\"points\", \"countryIndex\"))\n",
    "            .setOutputCol(\"features\")\n",
    "            .setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbt: org.apache.spark.ml.regression.GBTRegressor = gbtr_e0db4b568ef6\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gbt = new GBTRegressor()\n",
    "            .setLabelCol(labelColumn)\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setPredictionCol(\"Predicted \" + labelColumn)\n",
    "            .setMaxIter(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(strIdx_4d94574d0867, vecAssembler_baa0c40228de, gbtr_e0db4b568ef6)\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stages = Array(\n",
    "            countryIndexer,\n",
    "            assembler,\n",
    "            gbt\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_0cd78c344e64\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_0cd78c344e64\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [country: string, points: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------------+----------+------------------+\n",
      "|country|points|price|countryIndex|  features|   Predicted price|\n",
      "+-------+------+-----+------------+----------+------------------+\n",
      "|     US|  80.0| 12.0|         0.0|[80.0,0.0]|24.748344366963156|\n",
      "|     US|  80.0| 22.0|         0.0|[80.0,0.0]|24.748344366963156|\n",
      "|     US|  80.0| 25.0|         0.0|[80.0,0.0]|24.748344366963156|\n",
      "|     US|  80.0| 30.0|         0.0|[80.0,0.0]|24.748344366963156|\n",
      "|     US|  80.0| 36.0|         0.0|[80.0,0.0]|24.748344366963156|\n",
      "|     US|  80.0| 40.0|         0.0|[80.0,0.0]|24.748344366963156|\n",
      "|     US|  81.0|  8.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 13.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 16.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 19.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 19.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 20.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 22.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 25.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 29.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 29.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 29.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 30.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 37.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "|     US|  81.0| 48.0|         0.0|[81.0,0.0]|22.990867585442828|\n",
      "+-------+------+-----+------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions_na: org.apache.spark.sql.DataFrame = [country: string, points: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_na = predictions.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_5d5a5f14103f\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new RegressionEvaluator()\n",
    "            .setLabelCol(labelColumn)\n",
    "            .setPredictionCol(\"Predicted \" + labelColumn)\n",
    "            .setMetricName(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winePredError: Double = 27.66570715559826\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val winePredError = evaluator.evaluate(predictions_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
