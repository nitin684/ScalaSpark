{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://FireBall:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1548450385342)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 4\r\n",
       "y: Int = 7\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 4\n",
    "val y = 7\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkContext@56960008\n"
     ]
    }
   ],
   "source": [
    "println(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "z: Int = 11\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val z = x+y\n",
    "println(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkContext\r\n",
       "import org.apache.spark.SparkConf\r\n",
       "textFile: org.apache.spark.sql.Dataset[String] = [value: string]\r\n",
       "linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]\r\n",
       "res5: Long = 20\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val textFile = spark.read.textFile(\"README.md\")\n",
    "val linesWithSpark = textFile.filter(line => line.contains(\"Spark\"))\n",
    "textFile.count()  \n",
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "defined object SimpleApp\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "object SimpleApp {\n",
    "  def main(args: Array[String]) {\n",
    "      println(\"Starting Main method\")\n",
    "    val logFile = \"README.md\" // Should be some file on your system\n",
    "    val spark = SparkSession.builder.appName(\"Simple Application\").getOrCreate()\n",
    "    val logData = spark.read.textFile(logFile).cache()\n",
    "    val numAs = logData.filter(line => line.contains(\"a\")).count()\n",
    "    val numBs = logData.filter(line => line.contains(\"b\")).count()\n",
    "    println(s\"Lines with a: $numAs, Lines with b: $numBs\")\n",
    "    spark.stop()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26 02:39:32 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-01-26 02:39:32 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "RMSE: 5021.899441990948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@5c8861f6\r\n",
       "import sqlContext.implicits._\r\n",
       "df: org.apache.spark.sql.DataFrame = [User_ID: int, Product_ID: string ... 10 more fields]\r\n",
       "df1: org.apache.spark.sql.DataFrame = [User_ID: int, Occupation: int ... 2 more fields]\r\n",
       "import org.apache.spark.ml.feature.RFormula\r\n",
       "formula: org.apache.spark.ml.feature.RFormula = RFormula(Purchase ~ User_ID+Occupation+Marital_Status) (uid=rFormula_6a5f66545965)\r\n",
       "train: org.apache.spark.sql.DataFrame = [User_ID: int, Occupation: int ... 4 more fields]\r\n",
       "import org.apache.spark.ml.regression.LinearRegression\r\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_7c1eeafabf83\r\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_7c1eeafabf83\r\n",
       "trainingSumma..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "val df = sqlContext.read.format(\"com.databricks.spark.csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(\"train.csv\")\n",
    "\n",
    "df.columns\n",
    "\n",
    "//df.count()\n",
    "\n",
    "//df.printSchema()\n",
    "\n",
    "//df.show(2)\n",
    "\n",
    "//df.select(\"Age\").show(10)\n",
    "\n",
    "//df.filter(df(\"Purchase\") >= 10000).select(\"Purchase\").show(10)\n",
    "\n",
    "//df.groupBy(\"Age\").count().show()\n",
    "\n",
    "df.registerTempTable(\"B_friday\")\n",
    "\n",
    "//sqlContext.sql(\"select Age from B_friday\").show(5)\n",
    "\n",
    "val df1 = df.select(\"User_ID\",\"Occupation\",\"Marital_Status\",\"Purchase\")\n",
    "\n",
    "import org.apache.spark.ml.feature.RFormula\n",
    "val formula = new RFormula()\n",
    ".setFormula(\"Purchase ~ User_ID+Occupation+Marital_Status\")\n",
    ".setFeaturesCol(\"features\")\n",
    ".setLabelCol(\"label\")\n",
    "\n",
    "val train = formula.fit(df1).transform(df1)\n",
    "\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "val lr = new LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(train)\n",
    "\n",
    "val trainingSummary = lrModel.summary\n",
    "/* Now, See the residuals for train's first 10 rows. */\n",
    "//trainingSummary.residuals.show(10)\n",
    "\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
